---
description: Guidelines for implementing and maintaining tests for Task Master CLI and Agency Swarm projects.
globs: 
alwaysApply: false
---

# Testing Guidelines for Agency Swarm Projects

*Note:* When testing AI agent interactions, focus on verifiable inputs and outputs. Mock external dependencies like APIs thoroughly.

## Agency Swarm Specific Testing Considerations

When testing projects built with the Agency Swarm framework:

-   **Test Individual Tools First:** Thoroughly test each agent tool (Python classes inheriting from `BaseTool`) in isolation before testing the integrated agency interactions. Use the `if __name__ == "__main__":` block in each tool file for simple test cases.
-   **Utilize Evals for AI Behavior:** For tasks involving AI agent outputs or decisions, consider using specific evaluation methods (Evals) to verify behavior:
    -   **LLM Evaluator:** Checks text output against natural language criteria. Use `llm_validator` method from `agency_swarm.util.validators`.
    -   **Exact Match:** Verifies the presence/absence of specific strings.
    -   **Semantic Similarity:** Compares output to examples for factual/behavioral accuracy.
    -   **Function Call Presence:** Confirms specific agent tool/function calls were made.
    -   **File Output:** Validates generated files against expected content.
-   **Define Evals in Tasks:** When generating tasks (especially via AI or PRD parsing), ensure the `testStrategy` field explicitly mentions which specific Evals are required for verification. (See [`tasks.mdc`](mdc:.cursor/rules/tasks.mdc))

## General Test Organization Structure

- **Unit Tests**
  - Test individual functions and classes (like Agent Tools) in isolation.
  - Mock external dependencies (APIs, databases).
  - Keep tests focused and fast.

- **Integration Tests**
  - Test interactions between agents or agents and their tools.
  - Focus on the interfaces and communication flows.

- **End-to-End Tests**
  - Test complete agency workflows from user input to final output.
  - Simulate real-world scenarios.

- **Test Fixtures**
  - Provide reusable test data (e.g., sample inputs, expected outputs).
  - Keep fixtures organized and representative.

## Test Quality Guidelines

- ✅ **DO**: Write tests for critical functionality.
- ✅ **DO**: Test edge cases and error conditions.
- ✅ **DO**: Keep tests independent and isolated.
- ✅ **DO**: Use descriptive test names.
- ✅ **DO**: Maintain test fixtures separately.

- ❌ **DON'T**: Test implementation details that might change frequently.
- ❌ **DON'T**: Write brittle tests dependent on exact wording of complex AI outputs (use Evals instead).
- ❌ **DON'T**: Skip testing error handling.
- ❌ **DON'T**: Let tests become outdated.

## Running Tests (Agency Swarm)

For Agency Swarm projects, testing typically involves standard Python testing frameworks like `pytest`.

```bash
# Example using pytest (adapt as needed)
pytest  # Run all tests discovered by pytest

pytest path/to/specific_test_file.py  # Run tests in a specific file

pytest -k "test_function_name"  # Run tests matching a keyword expression
```